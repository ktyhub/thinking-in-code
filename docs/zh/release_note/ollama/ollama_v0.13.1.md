# ollama v0.13.1
## 为什么要使用Ollama

你是否厌倦了在云端AI的迷宫中徘徊？受够了高昂的费用、迟滞的响应，以及那份将敏感数据托付给未知服务器的隐隐不安？我们正站在一个矛盾的十字路口：一方面，我们渴望大语言模型（LLM）那近乎魔法的创造力与智能；另一方面，我们又因成本、隐私和延迟而却步，仿佛被一道无形的屏障隔绝在魔法之外。

Ollama的出现，正是为了击碎这面墙。它不是什么遥不可及的未来科技，而是一把今天就能握在手中的钥匙。**使用Ollama，意味着你将夺回控制权**。它让你能在自己的笔记本、工作站甚至家庭服务器上，轻松运行诸如Llama 3、Mistral、Qwen等强大的开源模型。想象一下：无需网络连接，零延迟地与你私人的AI助手对话；无需担心API调用次数，无限次地探索与迭代；你的每一次提示、每一份数据，都只留在你的硬盘上，成为绝对的隐私。

这不仅是技术的选择，更是一次立场的宣言：在AI民主化的浪潮中，你是选择继续做被动的服务消费者，还是成为主动的构建者和主宰者？Ollama邀请你选择后者。

## Ollama是什么

简而言之，**Ollama是一个让你能在本地计算机上一键下载、运行和管理开源大语言模型的工具**。

你可以把它想象成 macOS 上的 Homebrew，或是 Python 领域的 Pip，但它是专门为大型AI模型而生的。它通过简单的命令行，将复杂的模型部署、运行过程封装成诸如 `ollama run llama3` 这样的直观指令，让任何人都能轻松在本地启动一个功能完备的AI对话界面或API服务。

## 入门示例

让我们置身于一个真实的场景：

**场景**：你是一名独立开发者，想为你正在开发的新概念笔记应用添加一个“智能总结”功能。你希望它能快速理解用户输入的长篇会议记录，并提炼出核心要点和待办事项。使用云服务API要么太贵，要么无法满足数据不离线的安全要求。

**行动**：
1.  **安装**：从Ollama官网下载对应你操作系统（macOS、Linux、Windows）的安装包，几分钟内完成安装。
2.  **拉取模型**：打开终端，输入命令 `ollama pull qwen2.5:7b`。这个命令会从Ollama的模型库中下载性能优异且轻量的通义千问2.5 7B参数模型。
3.  **运行与交互**：下载完成后，输入 `ollama run qwen2.5:7b`。瞬间，一个交互式聊天界面在终端中打开。
4.  **开发集成**：你不必一直待在命令行里。在同一终端启动模型服务后，它同时会在 `http://localhost:11434` 提供一个与OpenAI API兼容的接口。在你的笔记应用代码中，你可以这样调用：

```python
import requests
import json

def summarize_with_ollama(text):
    response = requests.post(
        'http://localhost:11434/api/generate',
        json={
            'model': 'qwen2.5:7b',
            'prompt': f'请将以下文本总结为不超过3个要点的清单和待办事项列表：\n{text}',
            'stream': False
        }
    )
    return response.json()['response']

# 使用示例
meeting_notes = “今天会议主要讨论了三个问题：首先是Q2财报显示营收增长15%但利润率下降，需要成本控制方案；其次，新产品‘星图’的发布日期从6月推迟到8月，需要更新市场计划；最后，团队决定启动远程办公效率调研，由人事部牵头下周五前出初步报告。”
summary = summarize_with_ollama(meeting_notes)
print(summary)
```

顷刻之间，一个完全运行在你本地、由你掌控的AI大脑，就为你的应用注入了智能。没有账单，没有网络延迟，只有纯粹的创造力和控制力。

## Ollama v0.13.1版本更新了什么

根据官方发布日志，v0.13.1版本主要带来了以下关键改进：
1.  将 `nomic-embed-text` 嵌入模型的默认运行引擎切换为 Ollama 自家引擎，有望提升其性能和整合度。
2.  为 `cogito-v2.1` 模型新增了工具调用功能支持，增强了该模型的复杂任务处理能力。
3.  修复了CUDA VRAM（显存）发现机制的相关问题，提升了在NVIDIA GPU环境下的稳定性和资源识别准确性。
4.  修正了Ollama应用程序中指向文档的链接，改善了用户体验。
5.  此次更新属于小版本迭代，主要聚焦于特定模型的功能增强和已知问题的修复。

## 更新日志
### What‘s Changed
*   `nomic-embed-text` 模型现在将默认使用 Ollama 自身的引擎。
*   增加了对 `cogito-v2.1` 模型的工具调用功能支持。
*   修复了 CUDA VRAM 发现相关的问题。
*   修正了 Ollama 应用程序中指向文档的链接。

**完整更新日志**: [v0.13.0...v0.13.1-rc0](https://github.com/ollama/ollama/compare/v0.13.0...v0.13.1-rc0)

## 总结
简而言之，Ollama v0.13.1 是一次聚焦于“优化与修复”的更新，核心在于提升特定模型（`nomic-embed-text` 和 `cogito-v2.1`）的体验与能力，并解决了底层GPU资源管理的一个关键问题，使整个平台运行更为稳健顺畅。