# ollama v0.13.2
## 为什么要使用Ollama

在一个人工智能如同水电般嵌入我们生活的时代，我们却面临着一个巨大的矛盾：我们既渴望AI无所不能的智慧，又恐惧将最私密的想法、最核心的数据托付给远在云端的“黑箱”。我们享受ChatGPT们的便捷，却在深夜惊觉，每一次对话都可能成为训练他人的素材。这就是现代人的数字困境——在智能与隐私、便利与控制之间，被无形地撕裂。

而Ollama，正是对这个时代矛盾的直接回应。它不问你妥协，而是给你选择。它将巨人般的语言模型，从遥不可及的云端，请到了你的笔记本电脑、你的家用服务器上。使用Ollama，意味着你夺回了掌控权：你的数据在你自己的硬件上奔腾，你的对话不留痕迹于他人的服务器，你的灵感与思考，真正地只属于你。这不仅仅是一个技术工具，这是一次私人智能的“独立宣言”，让你在享受最前沿AI能力的同时，为自己的数字灵魂筑起一道坚固的围墙。

## Ollama是什么

简单来说，Ollama是一个让你能在自己的电脑上轻松运行、管理和操作各种大型语言模型（如Llama 3, Mistral, Gemma等）的框架。它把复杂的模型部署过程简化为几条命令，就像是为你打造了一个本地的、专属的AI服务器。

## 入门示例

想象你是一名独立开发者，正在构思一款为作家设计的深度写作辅助工具。你希望它能理解作者独特的文风，并提供极具个性化的剧情建议，但将用户宝贵的书稿上传至公开的AI服务让你顾虑重重。

这时，Ollama成为了你的基石。你可以在你的开发机（甚至是一台配备GPU的强力笔记本）上，通过几句简单的命令开始搭建：

1.  **安装与启动**：从官网下载Ollama并安装。打开终端，运行 `ollama run llama3.2`，系统便会自动为你拉取并启动一个强大的Meta Llama 3.2模型。一瞬间，一个本地的AI聊天界面就准备好了。
2.  **真实开发场景**：你不是只想聊天，你要把它集成到你的写作软件里。Ollama提供了与OpenAI兼容的API接口。你启动Ollama作为后台服务后，在你的Python代码中，可以像调用ChatGPT一样调用它，但数据完全不离本地：
    ```python
    import requests
    import json

    response = requests.post(
        ‘http://localhost:11434/api/generate’,
        json={
            ‘model’: ‘llama3.2’,
            ‘prompt’: ‘以海明威的风格，为一段关于老人在暴风雨前出海钓鱼的故事开头。’,
            ‘stream’: False
        }
    )
    result = response.json()
    print(result[‘response’])
    ```
3.  **效果**：你的写作应用现在拥有了一个理解力强、响应迅速、且绝对私密的AI大脑。作者可以尽情地探讨剧情、润色文字，而无需担心创意泄露。你以此为核心卖点，成功推出了你的产品。

## ollama v0.13.2版本更新了什么

本次更新主要包含两项关键修复和社区贡献者加入。它解决了在多GPU的CUDA环境中可能出现的GPU检测问题，确保了硬件资源的正确识别与利用。同时，修复了`deepseek-v3.1`模型在Ollama应用中被禁用“思考”功能后仍会执行该过程的问题，使模型行为更符合预期控制。

## 更新日志

### 更新内容
*   修复了在多GPU CUDA机器上的GPU检测问题。
*   修复了在Ollama应用中禁用“思考”功能后，`deepseek-v3.1`模型仍会执行思考过程的问题。

### 完整更新日志
详细变更记录可参阅：[v0.13.1...v0.13.2-rc0](https://github.com/ollama/ollama/compare/v0.13.1...v0.13.2-rc0)

## 总结

概括来说，版本v0.13.2是一个专注于修复和优化的更新，主要提升了在多GPU环境下的稳定性和对特定模型行为的控制准确性。